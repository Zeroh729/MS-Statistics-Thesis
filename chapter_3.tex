\chapter{Theoretical Framework}
\label{sec:theoreticalframework} 

\section{Target Quantification}
\label{sec:dpcr_ch3}

\subsection{Fluorescence, Concentration, and Dilution}
\label{sec:targetconc_ch3_stockDil}
Most molecules are in its state of lowest vibration level at room temperature, and its state becomes excited upon absorbing energy from light. This excitation elevates the molecule to higher vibration levels that cause the emission of fluorescence. The fluorescence intensity of dilute samples is related to physical variables such as the molecular extinction coefficient, quantum efficiency, intensity of incident light \cite{Elmer2000}.

When only a single fluorescent reporter is present, molecules emit only one type of fluorescence intensity. In the process of dPCR, fluorescent probes and primers attach to target sequences in the partitioned reaction mix. When the DNA molecules become excited, each droplet emits a fluorescence endpoint intensity, which is then used to identify if a droplet is positive or negative. An intensity threshold is then determined to classify droplets, such that any intensity less than the threshold is a negative droplet and positive otherwise \cite{Trypsteen2015}. The target concentration is then estimated using the counts of the classified droplets.

In analytical chemistry, the concentration, \(c\), is a measurement of the amount of solute present in an amount of solution,
\[
    c = \frac{\textup{amount of solute}}{\textup{amount of solution}},
\]
\noindent
where the fraction can be in terms of molarity (\(\frac{\textup{moles solute}}{\textup{liters solution}}\)), weight percent (\(\frac{\textup{mL solute}}{\textup{100 mL solution}}\)), weight-to-volume percent (\(\frac{\textup{grams solute}}{\textup{100 mL solution}}\)), etc \cite{Harvey2010}. Unless otherwise stated, concentrations in this paper are in terms of target molecule counts per \(\mu\)L. The concentration formulas specific to dPCR discussed in the following sections are referenced from \citeA{Kreutz2011}, \citeA{Zhu2014}, and \citeA{Gou2018}. 

The dilution factor, \(D\), is the ratio of the initial volume to the final diluted volume (\(V_1 : V_2\)), or equivalently, the ratio of the final diluted concentration to the initial stock concentration (\(c_2 : c_1\)).
\[
    D = \frac{V_1}{V_2} = \frac{c_2}{c_1}
\]

In reporting concentrations, the unknown stock concentration, \(c_1\), is the variable of interest. From the formula above, \(c_1\) can be obtained as 
\begin{equation}
    c_1 = c_2 \times \frac{1}{D}  \label{eq:c1}
\end{equation}

One approach to solve for \(c_1\) is to estimate \(c_2\), the target concentration from the diluted sample. Suppose that the average target copies per droplet  \(\lambda\) is given. Then, a simple unit conversion from \(\lambda\) to \(c_2\) (i.e. from target copies per droplet to target copies per \(\mu\)L) can be derived as follows 
\begin{equation}
    c_2 = \lambda \times \frac{1000}{V_{drp}},  \label{eq:c2}
\end{equation}
\noindent
where \(V_{drp}\) is the known constant droplet volume in terms of nL. Substituting \(c_2\) from equation \ref{eq:c2} to \ref{eq:c1}, and solving for \(\lambda\) brings
\begin{equation}
    \lambda = c_{1} \times \frac{V_{drp}}{1000} \times D, \label{eq:lambda_1}
\end{equation}
\noindent
which will be useful when a dilution series is available as discussed later on Section \ref{sec:targetconc_ch3_loglog}.

\subsection{Poisson Distribution in Counting Target Copies}
\label{sec:targetconc_ch3_poisson}
Let \(X\) be a random variable that represents the number of outcomes that appeared in either a time interval or a region of equal units \(h\) (specified as a time, distance, area, or volume).  \(X\) is defined to follow a Poisson distribution, when the following assumptions are satisfied:

\begin{enumerate}
    \item For all disjoint fixed time intervals or regions, the number of occurences in the span of \(h\) is independent from each other.
    \item The probability of only one outcome happening is proportional to the specified \(h\).
    \item The probability of more than one outcome given a small \(h\) is negligible relative to the probability of only one outcome occuring in the same space.
\end{enumerate}

The probability distribution function of \(X\) is defined as
\[
    P(x; \lambda) = \frac{e^{-\lambda}\lambda^{x}}{x!},\quad x = 0, 1, 2, ...
\]
where \(\lambda\) is the average number of outcomes per fixed time interval or region of \(h\) units \cite{Walpole2011}.

In the context of DNA quantification, the outcome of interest \(X\) is the number of target copies, and the region of fixed sizes \(h\) corresponds to a droplet of equal volumes \(V_{drp}\). The expected value of the target copies \(X\) per droplet, denoted by \(\lambda\) can be estimated using MLE as \(\hat{\lambda} = \frac{1}{n}\sum_{i=1}^{n}x_i\); where \(n\) is the number of independent trials, and \(x_i\) is 1 or 0 for a successful trial. By defining trial as a droplet, and success as a positive droplet, it follows that the probability of getting a positive droplet (i.e. at least one target copy) is \(P(x>0) = \frac{N_{pos}}{N_{tot}}\), where \(N_{tot}\) and \(N_{pos}\) is the count of the total and positive droplets, respectively. The equation below shows how \(\lambda\) can be estimated using Poisson probabilities 
\begin{equation}
    \begin{aligned}
        1-P(x=0) &= P(x>0)\\
        1-e^{-\lambda} &= \frac{N_{pos}}{N_{tot}}\\
        e^{-\lambda} &= 1-\frac{N_{pos}}{N_{tot}}\\
        \hat{\lambda} &= -\ln(1-\frac{N_{pos}}{N_{tot}})\\
        \hat{\lambda} &= -\ln(\frac{N_{neg}}{N_{tot}}) \label{eq:lambda} \\
    \end{aligned}
\end{equation}
Though the last two lines are obviously equivalent, it was derived here since the latter is the commonly used formula for \(\lambda\) \cite{Tzonev2018}. For this study, the comparison between quantification methods will be in terms of \(\hat{\lambda}\) concentration, rather than \(c_{1}\), since the latter is just \(\hat{\lambda}\) multiplied by some constants.

% TODO - Add lambda Confidence Interval formula

\subsection{Log-log Model in Limiting Dilution}
\label{sec:targetconc_ch3_loglog}
Serial dilution assays is a technique to estimate the target concentraton in a population; usually, the dilution factor (a level in the dilution series) progresses in a geometric sequence \cite{Deng2017}. For each dilution factor, sample replicates are prepared; producing a total of \(n\) assays. Let \(D_i\) denote the dilution factor at assay sample \(i\), where \(i={1,2,...,n}\). Then, continuing from equation \ref{eq:lambda_1}, for a given stock conconcentration \(c_1\) diluted by \(D_i\), the expected target copies per droplet is \(\lambda_i\). Thus, it can be said that with a fixed quantity \(c_1\), \(D_i\) is a predictor of \(\lambda_i\). Its relationship in equation \ref{eq:lambda_1} can then be linearized by taking the logarithm on both sides
\begin{equation}
    -\log{(\lambda)} = -\log{(c_1 \times \frac{V_{drp}}{1000})} - \log{(D)} \label{eq:loglambda}
\end{equation}

The proportion of target copies in the droplet population can then be estimated by fitting a binomial generalized linear model (GLM) with a log-log link:
\[
    g(\lambda_i) = \beta_0 + \log{(D_i)}.
\]

In GLM terms, \(\log{D_i}\) is the offset and \(g()\) is the complementary log-log link function resulting in the final model
\begin{equation}
    -\log{(\lambda_i)} = -\log{(c_1 \times \frac{V_{drp}}{1000})} - \log{(D_i)}\beta_1 \label{eq:loglog}
\end{equation}

Equation \ref{eq:loglog} can be formulated as a simple linear regression model \(Y = \beta_0 + X\beta_1 + \epsilon\), where \(\beta_0\) and \(\beta_1\) denotes the slope and intercept, respectively \cite{Walpole2011}. The error term \(\epsilon\) is a random variable assumed to be normally distributed with mean 0 and constant variance \(\sigma^2\). Given a set of ordered pairs {\((x_i,y_i);\) \(i=1,2,...n\)} and an estimated regression model \(\hat{y}_i = b_0 + x_ib_1\), the \(i\)th residual is defined as \(e_i = y_i - \hat{y}_i\). The ordinary least squares (OLS) estimator finds the values of \(b_0\) and \(b_1\) so as to minimize the residual sum of squares
\[
    \textup{SSRes} = \sum_{i=1}^{n}e_i = \sum_{i=1}^{n}(y_i - \hat{y}_i)^2.
\]

The OLS estimates of \(b_0\) and \(b_1\) for the regression coefficients \(\beta_0\) and \(\beta_1\) are 
\[
    b_1 = \frac{\sum_{i=1}^{n}(x_i-\bar{x})(y_i - \bar{y})}{\sum_{i=1}^{n}(x_i-\bar{x})^2}, \quad \textup{and}
\]
\[
    b_0 = \frac{\sum_{i=1}^{n}y_i-b_1\sum_{i=1}^{n}x_i}{n}.
\]

In assay analysis, the interpretation of a slope significantly greater than one implies that the proportion of the target sequence is hyper responsive to the diluted concentration. Otherwise, a slope less than one implies that the proportion of targets is less responsive to the diluted concentration, and suggests heterogeneity.  \cite{Hu2009}.


\section{Evaluation Metrics}
\label{sec:evalmetric_ch3}

\subsection{Precision of Technical Replicates}
\label{sec:evalmetric_ch3_cv}
% TODO : 
% 1. Explain Technical & Biological Variation - [https://www.sigmaaldrich.com/technical-documents/articles/biology/data-analysis.html](https://www.sigmaaldrich.com/technical-documents/articles/biology/data-analysis.html)

In quantitative assay studies, assay variability is typically summarized using the coefficient of variation (CV) \cite{Reed2003}. The CV of \(\hat{\lambda}\) is defined as
\[
    CV = \frac{SD(\hat{\lambda})}{mean(\hat{\lambda})}.
\]

A smaller CV implies good agreement amongst replicate estimates. The advantage of CV over standard deviation (SD) is that it takes into account the magnitude of the units, making CV comparable regardless of analyte concentration.

\subsection{Accuracy of Regression Model}
\label{sec:evalmetric_ch3_R2ResSE}
% TODO : 
% 1. Explain Technical & Biological Variation - [https://www.sigmaaldrich.com/technical-documents/articles/biology/data-analysis.html](https://www.sigmaaldrich.com/technical-documents/articles/biology/data-analysis.html)
% 2. Assuming tehcnical variation only, given a D_i, the expected is lambda_i

Recall that given an estimated regression line, the deviation of the fitted \(\hat{y}\) from the observed \(y\) is the error term \(\epsilon\) with mean 0 and variance \(\sigma^2\). The deviation of \(\epsilon\) measures the model's \emph{lack of fit} \cite{ISLR}. The residual standard error (RSE), or \(\hat{\sigma}\), is the estimated standard deviation of the error terms from fitting a regression model and is calculated as 
\[
    RSE = \sqrt{\frac{1}{n-2}\; \times \textup{SSRes}}.
\]

When the model predicts values that are very close to the observed data, such that \(\hat{y}_i \approx y_i\) for all \(i=1,2,...,n\), then RSE will be very small. On the other hand, predicted values that are far from the actual data will have a large RSE, indicating a poor fit. Since the magnitude of RSE depends on the units of \(Y\), it is not comparable between datasets, and also it is unclear what defines an acceptable RSE. 

In addition to RSE for assessing model accuracy, the coefficient of determination, \(R^2\), is unitless and is in the form of a proportion 
\[
    R^2 = \frac{\textup{SSTotal - SSRes}}{\textup{SSTotal}} = 1-\frac{\textup{SSRes}}{\textup{SSTotal}}.
\]
where \(\textup{SSTotal} = \sum (y_i - \bar{y})^2\) is the total variance in the response \(Y\). Since SSRes is equivalent to the amount of unexplained variance from the regression model, then in contrast, the interpretation of \(R^2\) is the proportion of variability in \(Y\) that can be explained by \(X\). An \(R^2\) close to 1 means that the regressor \(X\) explained a large percentage in the variability in \(Y\), and a value close to 0 means \(X\) does not explain much of the variability in the response.

%These numbers can then be totaled, yielding both a grand total and marginal totals. Totaling the entire table, the number of true positives, false negatives, true negatives, and false positives add up to 100% of the set. Totaling the rows (adding horizontally) the number of true positives and false positives add up to 100% of the test positives, and likewise for negatives. Totaling the columns (adding vertically), the number of true positives and false negatives add up to 100% of the condition positives (conversely for negatives). The basic marginal ratio statistics are obtained by dividing the 2×2=4 values in the table by the marginal totals (either rows or columns), yielding 2 auxiliary 2×2 tables, for a total of 8 ratios. These ratios come in 4 complementary pairs, each pair summing to 1, and so each of these derived 2×2 tables can be summarized as a pair of 2 numbers, together with their complements. Further statistics can be obtained by taking ratios of these ratios, ratios of ratios, or more complicated functions.

\subsection{Binary Classification Metrics}
\label{sec:evalmetric_ch3_binclass}
\begin{figure}[h]
    \centering
    \includegraphics[max size={\textwidth}{\textheight}]{confusionmatrix.png}
    \caption[Confusion matrix with TPR and FPR calculations]{Confusion matrix with TPR and FPR calculations}
        \label{fig:confusionmatrix}
\end{figure}

Most binary classification metrics are founded on the contingency table of ground truth vs predicted conditions, resulting in a tally of the true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN). This table in Figure \ref{fig:confusionmatrix} is also known as the confusion matrix, from which, ratios of cell counts, marginal (row or column) totals, or grand totals can be derived and are used as binary classification metrics. For this study, the classifier metrics False Positive Rate (FPR) and True Positive Rate (TPR) will be assessed. These two statistics are commonly assessed synchronously on a TPR vs FPR plot, called the receiver operating characteristics (ROC) graph. When TPR and FPR pairs are produced from multiple sample sets (or a range of threshold settings), a curve is usually formed in the ROC graph which represents the tradeoff of detection errors (false positives) and benefits (true positives). A good classifier should have more benefits than errors in most cases; that means, in the ROC graph, it is ideal that the points lie very close on the top left corner (1,1), producing a curve that largely covers the graph \cite{Fawcett2006}.
%However, it is not efficient to visually compare ROC curves from several classifiers; what is usually done is to evaluate a single scalar value called the area under the ROC curve (AUC). Since the AUC is bounded by the area of the ROC plot (a unit square), then AUC values fall within 0 to 1.


% Estimated conc as predictor for True conc %
\section{EM Clustering}
\label{sec:modelbased_ch3}

\subsection{G-component Finite Mixture Model}
\label{sec:gcomponentmixturedensity}
Denote \(X=\{x_1, ..., x_N; \myspace x_i \in \mathbb{R}^P\}\) as a statistically independent observation sequence where \(N\) is the number of observations and \(P\) is the dimensionality of \(x_i\), then the G-component finite mixture density is defined as 
\[
    f(X|\theta) = \sum_{g=1}^{G} \pi^{(g)} f^{(g)}(X; \phi^{(g)}),
\]
where \(\theta=\{\pi, \phi\}\) is the unknown parameter set, \(\pi^{(g)}\) is the mixing proportion \((\pi^{(g)} > 0, \sum_{g=1}^{G} \pi^{(g)}=1\)), \(f^{(g)}(X; \phi^{(g)})\) is the \(g\)th component density where \(\phi^{(g)}\) are its set of parameters. In most cases, the distribution is the same for all components, and so \(f^{(g)}(\cdot)\) can just be written as $f(\cdot)$ \cite{Mcnicholas2016}.

\subsection{Parameter Estimation Using EM}
\label{sec:em}
% TODO - ME
% The Expectation-Maximization (EM) algorithm is an approach to finding a maximized likelihood function set \(\theta^*\) for a G-component mixture model. The algorithm starts by guessing the parameter sets, and then iteratively maximizing these parameters using the E-step and M-step until the estimates converge. The EM algorithm are described more in the following :
% \begin{enumerate}
%     \item \(\mathit{Initialize: }\) Provide an initial guess for the unknown parameter set \(\theta_0\) and set \(t=0\) and \(Q(\theta_0|\theta_{-1})=-\infty\).
    
%     \item \(\mathit{E-step: }\) Compute the expected value of the log-likelihood function of \(\theta\) with respect to the current conditional distribution of \(z\) given \(x\) and the current estimates of the parameter \(\theta_t\).
%     \begin{equation}        
%         \begin{array}{l@{} >{\displaystyle}l}
%         Q(\theta|\theta_t) & = E_z[\log{(f(Z,X|\theta))} | X, \theta_t] \\ 
%                            & = \prod_{i=1}^{N}\sum_{g=1}^{G}E[\delta_{gi}|x_i,\theta_t] \log(f_g(x_i|\delta_{gi}=1, \psi_g) \pi_g) \\ 
%                            & = \prod_{i=1}^{N}\sum_{g=1}^{G}h_{gt}(x_i) \log(f(x_i|\delta_{gi}=1, \psi_g) \pi_g) \\ 
%         \end{array}
%     \end{equation}
    
%     The posterior probability membership function of step \(t\) is computed as
%         \begin{equation}
%             \displaystyle
%             h_{gt}(x_i) = \frac{\pi_{gt}f_g(x_i|\delta_{gi}=1,\psi_{gt})}{\sum_{k=1}^{G}\pi_{kt}f_k(x_i|\delta_{ki}=1,\psi_{kt})}
%         \end{equation}

%     \item \(\mathit{M-step: }\) Determine the value of \(\theta_{t+1}\) which maximizes \(Q(\theta|\theta_t)\),
%     \[
%         \theta_{t+1}=\underset{g\epsilon G}{\mathrm{argmax}}{(Q(\theta|\theta_t))}
%     \] 
%     Deriving this value for the parameter \(\pi_{g,t+1}\) can be done by maximizing \(Q(\theta|\theta_t)\) with respect to \(\pi_{g,t+1}\) by \(\frac{\partial Q(\theta|\theta_t)}{\pi_{gt}}=0\), while subject to the constraint of \(\sum_{g=1}^{G}\pi_g=1\). 
%     This finally yields to \(\pi_{g,t+1}=\frac{1}{N}\sum_{i=1}^{N}h_{gt}(x_i)\).

%     \item If \(Q(\theta_{t+1}|\theta_{t}) - Q(\theta_{t}|\theta_{t-1}) \le \xi \) (\(\xi \) is the specified termination threshold), then proceed to the last step. Otherwise, go back to Step 2.
    
%     \item The final parameter set \(\theta^* = \theta_{t+1}\) is the derived maximized likelihood estimate of \(\theta\).
% \end{enumerate}

In modeling a mixture of densities, the parameter estimation faces the problem that an observation's component membership is not known or is ``hidden'', and this complicates the structure of the likelihood function \cite{Kung2004}. Data with ``hidden'' or latent variables are considered as incomplete-data. One approach to solving the likelihood of an incomplete-data is by introducing the complete-data, the data where the component memberships are known for each observation. The Expectation-Maximization (EM) is an iterative procedure that computes the MLE of an incomplete-data by exploiting the complete-data likelihood. In the following sections, the EM theory is summarized from \citeA{McLachlan2019} and \citeA{Kung2004}, first by introducing the dependence of the incomplete-data likelihood to the complete-data likelihood, and then the iterative steps of EM and its convergence property.


\subsubsection{Incomplete-data Likelihood}
\label{sec:incompletedataLL}
To demonstrate this dependence, the following notations are continued from the previous section:
\begin{itemize}
    \item \(X=\{x_1, ..., x_N; \myspace x_i \in \mathbb{R}^P\}\) denotes the set of observations
    \item \(C=\{C^{(1)}, ..., C^{(G)}\}\) is the set of cluster labels for each component \(g\)
    \item \(Z=\{z_1, ..., z_N; \myspace z_i \in C^{(g)}\}\) denotes the set of the observations' component membership
    \item \(\theta_t\) is the parameter set estimate in iteration \(t\) 
\end{itemize}
Then the parameter set \(\theta_t\) can have a complete-data likelihood function defined as
\begin{equation}
    p(Z,X|\theta_t) = \prod_{i=1}^{N}p(z_i, x_i|\theta_t),
\end{equation}
then by probability theory, the likelihood of the incomplete-data can be expressed using the complete-data likelihood :
\begin{equation}
    p(X|\theta_t) = \frac{p(Z,X|\theta_t)}{P(Z|X,\theta_t)}.
\end{equation}
Based on \citeA{Kung2004}, the incomplete-data log-likelihood can be obtained as a sum of two terms derived as follows :
\begin{equation*}
    \begin{aligned}
    L( \theta _{t}) & =\log( p( X|\theta _{t}))\\
     & =\log( p( X|\theta _{t}))\underbrace{\sum _{Z} P( Z|X,\theta _{t})}_{( 1)}\\
     & =\sum _{Z}\log( p( X|\theta _{t})) P( Z|X,\theta _{t})\\
     & =\sum _{Z}\log\left(\frac{p( X,Z|\theta )}{P( Z|X,\theta _{t})}\right) P( Z|X,\theta _{t})\\
     & =\sum _{Z}\log( p( X,Z|\theta _{t})) P( Z|X,\theta _{t}) \ -\ \sum _{Z}\log( P( Z|X,\theta _{t})) P( Z|X,\theta _{t})\\
     & =E_{Z}[ log( p( X,Z|\theta _{t}) |X,\theta _{t}] \ -\ E_{Z}[ log( P( Z|X,\theta _{t}) |X,\theta _{t}]\\
     & =Q( \theta _{t} |\theta _{t}) \ +\ R( \theta _{t} |\theta _{t})
    \end{aligned}
\end{equation*}

By denoting \(Q(\theta | \theta_{t}) \equiv E_{Z}[\log{(p(Z, X | \theta_{t}))}| X, \theta_{t}]\) as the expectation of the complete-data log-likelihood given \(X\) using the parameter set of step \(t\), and \(R(\theta | \theta_{t})\) as the entropy term, then the incomplete-data likelihood maximization problem can be accomplished by maximizing \(Q(\theta | \theta_{t})\) or \(R(\theta | \theta_{t})\). Whereas the latter can be achieved by performing simulated annealing, \(Q(\theta | \theta_{t})\) can be maximized using the convergence property demonstrated in the following:
\begin{equation}
\begin{aligned}
L( \theta ) & =\log( p( X|\theta )) & \\
 & =\log\left(\sum _{Z} p( X,Z|\theta )\underbrace{\frac{P( Z|X,\theta _{t})}{P( Z|X,\theta _{t})}}_{( 1)}\right) & \\
 & =\log\left(\sum _{Z}\frac{p( X,Z|\theta )}{P( Z|X,\theta _{t})} P( Z|X,\theta _{t})\right) & \\
 & =\log\left( E_{Z}\left[\frac{p( X,Z|\theta )}{P( Z|X,\theta _{t})} \ |\ X,\theta _{t}\right]\right) & \\
 & \geqslant E_{z}\left[\log\left(\frac{p( X,Z|\theta )}{P( Z|X,\theta _{t})}\right) \ |\ X,\theta _{t}\right] & \text{(by Jensen's inequality)}\\
 & \geqslant E_{z}[\log( p( X,Z|\theta )) \ |\ X,\theta _{t}] \ -\ E_{z}[\log( P( Z|X,\theta _{t})) \ |\ X,\theta _{t}] & \\
 & \geqslant Q( \theta |\theta _{t}) +R( \theta |\theta _{t}) & 
\end{aligned}
\label{eq:emconvergence}
\end{equation}

Suppose $\displaystyle \theta ^{*}$ is an element in the parameter space and it is given to maximize $\displaystyle Q$, that is:
\begin{equation*}
    \theta ^{*} =\underset{\theta }{\arg\max} Q( \theta |\theta _{t})
\end{equation*}
Continuing the inequality from \ref{eq:emconvergence},
\begin{equation*}
    \begin{aligned}
        L\left( \theta ^{*}\right) & \geqslant Q\left( \theta ^{*} |\theta _{t}\right) +R( \theta _{t} |\theta _{t})
    \end{aligned}
\end{equation*}
And since $\displaystyle \ Q\left( \theta ^{*} |\theta _{t}\right)  >Q( \theta _{t} |\theta _{t})$, then it follows that
\begin{equation}
    \begin{aligned}
        L\left( \theta ^{*}\right) & \geqslant Q( \theta _{t} |\theta _{t}) \ +\ R( \theta _{t} |\theta _{t})\\
        L\left( \theta ^{*}\right) & \geqslant L( \theta _{t})
    \end{aligned}
    \label{eq:LLinequality}
\end{equation}
This shows that for a given parameter set $\displaystyle \theta _{t}$ at iteration $\displaystyle t$, a $\displaystyle \theta ^{*}$ can be chosen that will increase the incomplete-data log-likelihood $L(\theta_t)$.


\subsubsection{Complete-data Likelihood}
\label{sec:completedataLL}
To assist in the derivation of the complete-data likelihood, the indicator variable \(\delta_{gi}\) was adopted from the notation of \citeA{Kung2004} and is introduced to express the status of the hidden states as
\begin{equation*}
    \delta ^{( g)}_{i} =\delta \left( z_{i} \ ,C^{( g)}\right) =\begin{cases}
    1 & \text{if} \ x_{i} \ \text{was generated by } C^{( g)}\\
    0 & \text{otherwise}
    \end{cases}
\end{equation*}
Using the indicator variable, the complete-data likelihood is given by
\begin{equation*}
    \begin{array}{ r r l }
     & p( X,Z|\theta ) \  & {\displaystyle =\prod ^{N}_{i=1}\sum ^{G}_{g=1} \delta ^{( g)}_{i} \ p\left( x_{i} ,\ z_{i} =C^{( g)} \ |\ \theta \right)}\\
     &  & {\displaystyle =\prod ^{N}_{i=1}\sum ^{G}_{g=1} \delta ^{( g)}_{i} \ p\left( x_{i} ,\ \delta ^{( g)}_{i} =1\ |\ \theta \right)}\\
    L_{C}( \theta ) \ \equiv  & \log( p( X,Z|\theta )) & {\displaystyle =\sum ^{N}_{i=1}\log\left(\sum ^{G}_{g=1} \delta ^{( g)}_{i} \ p\left( x_{i} ,\ \delta ^{( g)}_{i} =1\ |\ \theta \right)\right)}\\
     &  & {\displaystyle =\sum ^{N}_{i=1}\log\left(\sum ^{G}_{g=1} \delta ^{( g)}_{i} \ p\left( x_{i} \ |\ \delta ^{( g)}_{i} =1,\theta \right) \ p\left( \delta ^{( g)}_{i} =1\ |\ \theta \right)\right)}
    \end{array}
\end{equation*}
To continue the equation, it is noted that:
\begin{itemize}
    \item when expanding the summation \(\sum_{g=1}^{G}\), there would only be one nonzero term. And therefore \(\delta^{g}_i\) can be pulled out from the log function without affecting the results;
    \item $\displaystyle p\left( x_{i} |\delta ^{( g)}_{i} \ =\ 1,\ \theta \right) \ =\ p\left( x_{i} |\delta ^{( g)}_{i} \ =\ 1,\ \phi ^{( g)}\right)$
    \item $\displaystyle p\left( \delta ^{( g)}_{i} \ =\ 1\ |\ \theta \right) \ =\ \pi ^{( g)}$
\end{itemize}
By putting these all together, the complete-data log-likelihood $L_{C}( \theta )$ is finally expressed as
\begin{equation}
    L_{C}( \theta ) \ =\sum ^{N}_{i=1}\sum ^{G}_{g=1}{\displaystyle \delta ^{( g)}_{i} \ }\log\left({\displaystyle p\left( x_{i} \ |\ \delta ^{( g)}_{i} =1,\phi ^{( g)}\right) \ \pi }^{( g)}\right)
    \label{eq:completedataLL}
\end{equation}

The EM algorithm is an iterative algorithm that can maximize $L(\theta)$, by maximizing $L_C(\theta)$, at each iteration of the expectation step (E-step) and the maximization step (M-step). To start the algorithm, $t$ is initialized to $0$ and values for the initial parameter set $\theta_0$ are given. Then E-step and M-step execute alternatively, incrementing $t$ for each loop and updating $\theta_t$ until $L(\theta)$ converges. The E-step computes the expectation of the likelihood of the mixture model parameters by including the hidden variables. The M-step re-estimates the parameters by maximizing the expected likelihood from the E-step. These new model parameters from the M-step are then used for the next iteration until the difference of $L(\theta_{t+1})$ and $L(\theta_{t})$ become arbitrarily small.

\subsubsection{Initialization}
\label{sec:em_initialization}
Before starting the first iteration ($t=0$), EM requires an initial guess of the parameter set for $\theta_0$. 

\subsubsection{E-step}
\label{sec:estep}
At iteration $t$, E-step computes the function $\displaystyle Q( \theta |\theta_{t})$ using the complete-data log-likelihood from equation \refeq{eq:completedataLL}
\begin{equation}
    \begin{aligned}
    Q( \theta |\theta _{t}) & =E_{Z}[ L_{C}( \theta ) \ |\ X,\ \theta _{t}]\\
     & =E_{Z}\left[{\displaystyle \sum ^{N}_{i=1}\sum ^{G}_{g=1} \delta ^{( g)}_{i} \ }\log\left({\displaystyle p\left( x_{i} \ |\ \delta ^{( g)}_{i} =1,\phi ^{( g)}\right) \ \pi }^{( g)}\right) \ |\ X,\ \theta _{t}\right]\\
     & =\sum ^{N}_{i=1}\sum ^{G}_{g=1} E_{Z}\left[{\displaystyle \delta ^{( g)}_{i} \ |\ x_{i} ,\ \theta }_{t}\right] \ \log\left({\displaystyle p\left( x_{i} \ |\ \delta ^{( g)}_{i} =1,\phi ^{( g)}\right) \ \pi }^{( g)}\right)\\
     & =\sum ^{N}_{i=1}\sum ^{G}_{g=1} P\left( \delta ^{( g)}_{i} =1\ |\ x_{i} \ ,\ \theta _{t}\right){\displaystyle \ }\log\left({\displaystyle p\left( x_{i} \ |\ \delta ^{( g)}_{i} =1,\phi ^{( g)}\right) \ \pi }^{( g)}\right)
    \end{aligned}
    \label{eq:qfunc}
\end{equation}
By denoting the component membership function $h^{( g)}_{t}( x_{i})$ as the probability of the latent variable $z_i$ to be the component label $C^{(g)}$ given the observation $x_i$ and parameter set $\theta_t$, its equation can be expressed using Bayes theorem as follows
\begin{equation}
    \begin{aligned}
    h^{( g)}_{t}( x_{i}) & \equiv E_{Z}\left[{\displaystyle \delta ^{( g)}_{i} \ |\ x_{i} ,\ \theta }_{t}\right]\\
     & =P\left( \delta ^{( g)}_{i} =1\ |\ x_{i} \ ,\ \theta _{t}\right)\\
     & =\frac{p\left( x_{i} \ |\ \delta ^{( g)}_{i} \ =\ 1,\ \theta _{t}\right) P\left( \delta ^{( g)}_{i} \ =\ 1\ |\ \theta _{t}\right)}{\sum\limits ^{G}_{k=1} p\left( x_{i} \ |\ \delta ^{( k)}_{i} \ =\ 1,\ \theta _{t}\right) P\left( \delta ^{( k)}_{i} \ =\ 1\ |\ \theta _{t}\right)}\\
     & =\frac{p\left( x_{i} \ |\ \delta ^{( g)}_{i} \ =\ 1,\ \phi ^{( g)}_{t}\right) \pi ^{( g)}_{t}}{\sum\limits ^{G}_{k=1} p\left( x_{i} \ |\ \delta ^{( k)}_{i} \ =\ 1,\ \phi ^{( g)}_{t}\right) \pi ^{( g)}_{t}}
    \end{aligned}
    \label{eq:clustermemfunc}
\end{equation}
Given $\theta_t$, the best guess of the component membership probability $h^{( g)}_{t}( x_{i})$ for all observations and components is determined, and by doing so, $\displaystyle Q( \theta |\theta _{t})$ is now then a function of $\theta$.

\subsubsection{M-step}
\label{sec:mstep}
At iteration $t$, M-step finds an adjusted $\theta_{t+1}$ that maximizes the function $\displaystyle Q( \theta |\theta _{t})$ with respect to $\theta$ over the parameter space $\Omega$. Recall that for a parameter set of component $g$, $\displaystyle \theta ^{( g)} =\left\{\phi ^{( g)} \ ,\ \pi ^{( g)}\right\}$, it consists of the  $g$th component mixing proportion $\pi^{( g)}$ and the density parameters $\phi^{( g)}$. The estimation of these two parameters are done sequentially and is described succeedingly.

The adjusted mixing proportion $\displaystyle \pi^{(g)}_{t+1}$ is calculated independently from $\displaystyle \phi ^{( g)}_{t+1}$; and that if the component membership \(z_i\) was observed, then the mixing proportion can be obtained using ML as 
\[\pi^{(g)}_{t+1}=\sum_{i=1}^{N}\delta_{i}^{(g)}/N.\]
Instead, the E-step provides a component membership probability $h^{(g)}_{t}(x_{i})$ to estimate $\delta_{i}^{(g)}$
\begin{equation*}
    \begin{aligned}
    \pi^{(g)}_{t} & =\sum ^{N}_{i=1} h^{( g)}_{t}( x_{i}) /N\\
     & =\sum ^{N}_{i=1} E\left[ \delta ^{( g)}_{i} \ |\ x_{i} \ ,\ \theta _{t}\right] \ /\ N
    \end{aligned}
\end{equation*}
Which means that the estimated $\pi^{(g)}_{t+1}$ is based on the posterior probabilities of $g$th component membership for all observations $X$.

Following the estimation of $\displaystyle \pi^{(g)}_{t+1}$, the only variable remaining to be maximally estimated in the function $Q(\theta | \theta_t)$ are the component density parameters $\phi^{(g)}_t$ for all $g$ (based on equation \ref{eq:qfunc}). The ML estimate of $\phi^{(g)}_{t+1}$ can be obtained as an appropriate root of 
\begin{equation*}
    \sum ^{G}_{g=1}\sum ^{N}_{i=1} h^{( g)}_{t}( x_{i}) \ \frac{\partial \log\left( p\left( x_{i} \ |\ \delta ^{( g)}_{i} =1,\ \phi ^{( g)}_{t}\right) \ \pi ^{( g)}_{t}\right)}{\partial \phi } =0
\end{equation*}

Finally, after obtaining the estimates $\theta_{t+1}$ that maximizes $Q(\theta|\theta_t)$, until the converge rule is satisfied, the iteration $t$ is incremented, thereby carrying over $\theta_{t+1}$ to the calculation of the next E-step.

\subsubsection{Convergence}
\label{sec:convergence}
After the M-step, it is checked if the incomplete-log likelihood has reached converge, that is
\begin{equation*}
    L(\theta_{t+1}) -L(\theta_{t}) \geqslant \epsilon ;
\end{equation*}
where $\epsilon$ is the specified termination threshold. As stated in equation \ref{eq:LLinequality}, the likelihood $\displaystyle L(\theta _{t})$ increases at each iteration, until it reaches a point where the improvement becomes arbitrarily small and is considered negligible. However, the convergence of the likelihood value from $L(\theta_{t})$ to $L^*$ does not imply that $\theta_{t}$ has reached its maximum point $\theta^*$ \cite{McLachlan2019}. In most cases, the limiting value $L^*$ is a local maximum. A common workaround for this problem is to perform EM in a variety of initial parameter sets.

\subsection{Model-based Clustering}
\label{sec:modelbasedclustering}
% The common method for assigning a cluster \(g\) to \(x_i\) is by using the maximum a posteriori (MAP) classification such that 
% \begin{equation}
%     \textup{MAP}(\delta_{gi}) = \left\{
%         \begin{matrix}
%             1 & \textup{ if } \textup{max}_g\{h_g(x_i)\} \textup{ is in cluster } C_g \\ 
%             0 & \textup{ otherwise }
%         \end{matrix}\right. ,
% \end{equation}
% \noindent
% where \(\textup{MAP}(\delta_{gi})\) of 1 means that amongst all the clusters in \(C\), \(x_i\) has the highest probability to belong to cluster \(C_g\), and is therefore assigned to \(C_g\).
Model-based clustering refers to the use of statistical procedures to infer and assign clusters to observations. A general methodology of model-based clustering involves fitting mixture models, model selection, and data clustering using the maximum a posteriori (MAP) rule \cite{Baudry2015, Bulla2013}. Often in cluster analysis, the estimated parameters from the mixture model is not of interest, but rather the final classification of the observations. The MAP rule enforces a hard classification to observation $x_i$ based on its highest conditional probability for all components $g$, that is
\begin{equation*}
    \hat{\delta }^{( g)}_{i} =\delta \left(\hat{z}_{i} ,C^{( g)}\right) =\begin{cases}
    1 & \text{if }\underset{k\in \{1,\dotsc ,G\}}{\arg\max}\; h^{( k)}( x_{i}) =g\ \\
    0 & \text{otherwise}
    \end{cases}
\end{equation*}
where $h^{( k)}( x_{i})$ is the component membership probability estimated from the E-step of EM (in equation \ref{eq:clustermemfunc}), and $\delta^{( g)}_{i}$ is the indicator function that represents the latent variable $z_i$ to be the component label $C^{(g)}$. By estimating the MAP $\hat{\delta }^{( g)}_{i}$, the conditional probabilities $h^{( k)}( x_{i})$ are disregarded and is unutilized in the succeeding cluster analysis. Other approaches to classification uses probability thresholds or fuzzy classification for when the study of interest is in the shape and overlaps of the mixture components \cite{Baudry2012}.

\section{Nonnormal Mixture Models}
\label{sec:nonnormalMixModels}

\subsection{T-Mixture Models}
\label{sec:tmix_ch3}
The G-component mixtures of t distributions can be written as
\begin{equation*}
f( x;\theta ) =\sum ^{G}_{g=1} \pi ^{( g)} f\left( x;\ \mu ^{( g)} ,\sigma ^{( g)} ,v^{( g)}\right)
\end{equation*}
where $\displaystyle \theta =\{\pi ,\ \phi ,\ v\}$ is the parameter set that contains a vector of mixing proportions, density parameters, and degrees of freedom, and that the $\displaystyle g$th component density parameters are $\displaystyle \phi ^{( g)} =\left\{\mu ^{( g)} ,\ \sigma ^{( g)}\right\}$. \citeA{Peel2000} defines the complete-data vector as 
\begin{equation*}
X_{C} =\{X,Z,U\}
\end{equation*}
	where $\displaystyle X\ =\{x_{1} ,\dotsc ,x_{N}\}$ is the observed data vector,
	$\displaystyle Z\ =\{z_{1} ,\dotsc ,z_{N}\}$ is the component-label vectors, and 
	$\displaystyle U\ =\{u_{1} ,\dotsc ,u_{N}\}$ are defined such that for independent observations $\displaystyle x_{i}$,
\begin{equation*}
x_{i} \ |\ u_{i} \ ,\delta ^{( g)}_{i} =1\ \sim N\left( \mu ^{( g)} ,\ \sigma ^{( g)} /u_{i}\right) ,\ 
\end{equation*}
\begin{equation*}
u_{i} \ |\ \delta ^{( g)}_{i} =1\ \sim Gamma\left(\frac{v^{( g)}}{2} ,\frac{v^{( g)}}{2}\right) ,
\end{equation*}
	and
\begin{equation*}
Gamma( u;\ \alpha ,\beta ) =\frac{\beta ^{\alpha }}{\Gamma ( \alpha )} u^{\alpha -1} e^{-\beta u} .
\end{equation*}
Due to the characteristics of the t-distribution, augmenting the observations $\displaystyle X$ with $\displaystyle Z$ is still incomplete. The introduction of the latent variable $\displaystyle U$ above aids in the computation of the complete-data likelihood ($\displaystyle L_{C}( \theta )$). According to \citeA{Peel2000}, $\displaystyle \log L_{C}( \theta )$ can be expressed as a sum of the logarithms of the marginal distribution of $\displaystyle z_{i}$, conditional densities of $\displaystyle u_{i}$ given $\displaystyle z_{i}$, and the conditional density of $\displaystyle x_{i}$ given $\displaystyle u_{i}$ and $\displaystyle z_{i}$. There are different approaches to finding the ML estimates of the t-mixture model, by observing that as $\displaystyle v\rightarrow \infty $, causing $\displaystyle U\rightarrow 1$ with probability 1 and $\displaystyle X$ to be normally distributed, one can set equal $\displaystyle v$ for all components or it can be inferred from the data for each component. EM variants for finding ML estimates of multivariate t-mixture models include the expectation-condition maximization either (ECME) \cite{Liu2002}, alternating expectation-conditional maximization (AECM) \cite{Meng1997}, and the parameter-expanded EM (PX-EM) for a generalized data augmentation for possible missing values \cite{Liu2002}.

\subsection{Skewed T-Mixture Models}
\label{sec:skewedtmix_ch3}
Data commonly observed in finance and cytometry exhibits asymmetric and nonnormal features. For these cases, distributions from the the skew-elliptical family can be considered which have additional parameters to describe the skewness, kurtosis, and other characterization of its densities.  One member that has received attention for its flexibility and tractability is the skew-t distribution, and variants of its distribution have been proposed from \citeA{Azzalini2003,Branco2001,Lachos2010, Iversen2010ClosedskewDS, Arellano2010} which may be classified as ``restricted",``unrestricted", ``extended", and ``generalized". The differences in the classifications are based on the specifications of the latent variable $X_0$ of which the skew t-distribution is obtained by conditioning $X_1$ with it, the dependence of $X_0$ and $X_1$, and the dimensions of the t-distribution. An adoption of the finite restricted multivariate skew t mixture model is given by
\begin{equation*}
    f( x;\theta ) =\sum\limits ^{G}_{g=1} \pi ^{( g)} f\left( x;\mu ^{( g)} ,\ \Sigma ^{( g)} ,\ \delta ^{( g)} ,\ v^{( g)}\right),
\end{equation*}
where the complete-data vectors consists of $\displaystyle X_{C} =\{X,\ Z,\ U,\ W\}$, $\displaystyle u_{i} \ |\ w_{i} \ ,\ z^{( g)}_{i} =1\ \sim Half\text{-} Normal( 0,1)$, and $\displaystyle w_{i} \ |\ z^{( g)}_{i} =1\ \sim Gamma\left(\frac{v^{( g)}}{2} ,\frac{v^{( g)}}{2}\right)$. 

\subsection{Generalized Hyperbolic Mixture Models}
\label{sec:ghd_ch3}
The generalized hyperbolic (GH) distribution, first introduced by Barndorff-Nielsen \cite{Barndorff1977}, is a continuous probability distribution with five parameters that describe its location, scale, asymmetry, and the decay of its tails. As the name suggests, this distribution is generalized and is a superclass of the normal inverse Gaussian distributions, scaled t-distributions, standard hyperbolic distributions, variance-gamma distributions, among others. The tails of the GH distribution can range from a Gaussian-like tail to a heavy tail of exponential behavior. Both tails can exhibit different behaviors simultaneously, where the left-hand can be less heavy than the right-hand tail. This property of the GH distribution allows the modeling of asymmetric heavy-tailed populations commonly observed in finance and econometric data \cite{Takahashi2016, Nwobi2014, Necula2009, Aas2006, Bibby2003}. Its applications are in predicting risk models of exchange rates, portfolios, and stock index returns data. 

% TODO - ME // rewrite to f(x|theta)
Let \(X\) be a random variable that follows a generalized hyperbolic distribution with parameters for location (\(\lambda\)), scaling (\(\delta\)), shape (\(\alpha\)), skewness (\(\beta\)), and a parameter \(\mu\) that influences kurtosis and the GH characterization. 
\[
    X \sim GH(\lambda, \alpha, \beta, \delta, \mu)
\]
Then the probability distribution function of \(X\) is defined as
\begin{equation}
    \begin{aligned}[l]
    P(x; \lambda, \alpha, \beta, \delta, \mu) = a(\lambda, \alpha, \beta, \delta, \mu) (\delta^2+(x-\mu)^2)^{1/2\lambda-1/4} \\
    \cdot \; B(\lambda-0.5, \alpha\sqrt{\delta^2+x^2-2x\mu+\mu^2})e^{\beta(x-\mu)}
    \end{aligned}
\end{equation}
where
\[
    a(\lambda, \alpha, \beta, \delta, \mu) = \frac{(\alpha^2-\beta^2)^{1/2\lambda}}{\sqrt{2\pi}\alpha^{\lambda-1/2}\delta^\lambda B(\lambda, \delta\sqrt{\alpha^2 - \beta^2})}
\]
and \(B(\lambda, \cdot)\) denotes the modified Bessel function of the third kind with index \(\lambda\).
GH distribution mixture models were assessed in a study of \citeA{Browne2015} using Old Faithful data \cite{GeyserTimes} and simulated datasets. The former dataset was observed to have skewed tails, and the resulting GH mixture model was shown to have a superior fit as compared to the scale mixture of skew-normal distributions. The simulated dataset was composed of one hundred 2-component mixtures of Gaussian and skew-t distributions. When a GH mixture model was fitted using EM algorithm, all the population parameters were very close to the true values. These demonstrate the ability of GH mixture model to closely capture real data consisting of several populations.
