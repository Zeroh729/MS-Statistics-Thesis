\chapter{Theoretical Framework}
\label{sec:theoreticalframework} 

\section{EM Clustering}
\label{sec:modelbased_ch3}

\subsection{G-component Finite Mixture Density}
\label{sec:gcomponentmixturedensity}
Denote \(X={x_1, ..., x_N; x_i \epsilon \mathbb{R}^P}\) as a statistically independent observation sequence where \(N\) is the number of observations and \(P\) is the dimensionality of \(x_i\).
The G-component Finite Mixture Density is \[f(X|\theta) = \sum_{g=1}^{G} \pi_g f_g(X|\psi_g)\];
where \(\pi_g\) is the mixing proportion \(\pi_g > 0, \sum_{g=1}^{G} \pi_g=1\)), \(\psi_g\) are its corresponding parameters, \(f_g(X|\psi_g)\) is the \(g\)th component density, and \(\theta={\theta_1, ..., \theta_G}\), \(\theta_g={\pi_g,\psi_g}\) is the unknown parameter set that defines the density function for approximating the true probability of \(X\).


\subsection{Model-based clustering}
\label{sec:modelbasedclustering}
Denote \(C={C_1, ..., C_G}\) is the set of cluster mixture labels. \(Z={z_1, ..., z_N; z_i \epsilon C}\) is the set of "hidden" states where \(z_i=C_g\) means that the \(g\)th mixture generated \(x_i\). 
\[\delta_{gi}=\delta(z_i,C_g)=\left\{\begin{matrix}
    1 & \textup{ if } x_i \textup{ is generated by mixture } C_g \\ 
    0 & \textup{ otherwise }
    \end{matrix}\right.\]

Suppose \(X={x_1, ..., x_N}\) of \(N\) \(p\)-dimensional data vectors are observed and all \(N\) are unlabelled or treated as unlabelled. The likelihood is then expressed as 
\[\begin{matrix*}[l]
    f(Z,X | \theta) & =\prod_{i=1}^{N}f(z_i, x_i | \theta) \\ 
                    & =\prod_{i=1}^{N} \sum_{g=1}^{G} \delta_{gi}f_g(z_i,x_i|\theta_g)\\ 
                    & =\prod_{i=1}^{N} \sum_{g=1}^{G} \delta_{gi}f_g(x_i,z_i=C_g|\theta_g) \\ 
                    & =\prod_{i=1}^{N} \sum_{g=1}^{G}\delta_{gi}f_g(x_i,\delta_{gi}|\theta_g)
    \end{matrix*}\]

Given a maximized parameter set \(\theta^*\), the membership function is the posterior probabilities \(h_g(x_i)=P(\delta_{gi}=1|x_i,\theta^*)\). The membership function is the probability of \(x_i\) belonging to the \(g\)-th cluster, given \(x_i\) and the model. This can be derived using Bayesian theorem 
\[\begin{matrix*}[l]
    h_g(x_i)& = P(\delta_{gi}=1|x_i,\theta^*) \\ 
            & = \frac{\pi_gf_g(x_i|\delta_{gi}=1,\psi_g)}{\sum_{k=1}^{G}\pi_kf_k(x_i|\delta_{ki}=1,\psi_k)}\\ 
    \end{matrix*}\]

The common criteria for assigning a cluster \(g\) to \(x_i\) is by finding the component \(g_i^*=\underset{g\epsilon G}{\mathrm{argmax}}(h_{g}(x_i))\).


\subsection{Expectation Maximization}
\label{sec:em}
An approach to finding a maximized likelihood function set \(\theta^*\) for a G-component mixture model is the Expectation Maximization (EM) algorithm. This procedure follow an iterative step:
\begin{enumerate}
    \item \(\mathit{Initialize: }\) Provide an initial guess for \(theta_0\). Set \(t=0\) and \(Q(\theta_0|\theta_{-1})=-\infty\).
    
    \item \(\mathit{E-step: }\) Compute the expected value of the log-likelihood function of \(\theta\) with respect to the current conditional distribution of \(z\) given \(x\) and the current estimates of the parameter \(\theta_t\).
    \[\begin{matrix*}[l]
        Q(\theta|\theta_t) & = E_z[\log f(Z,X|\theta) | X, \theta_t] \\ 
                           & = \prod_{i=1}^{N}\sum_{g=1}^{G}E[\delta_{gi}|x_i,\theta_t] \log(f_g(x_i|\delta_{gi}=1, \psi_g) | \pi_g) \\ 
                           & = \prod_{i=1}^{N}\sum_{g=1}^{G}h_{gt}(x_i) \log(f(x_i|\delta_{gi}=1, \psi_g) | \pi_g) \\ 
        \end{matrix*}\]
    
    The posterior probability membership function of step \(t\) is also computed as
        \[h_{gt}(x_i) = \frac{\pi_{gt}f_g(x_i|\delta_{gi}=1,\psi_{gt})}{\sum_{k=1}^{G}\pi_{kt}f_k(x_i|\delta_{ki}=1,\psi_{kt})}\]

    \item \(\mathit{M-step: }\) Determine the value of \(\theta_{t+1}\) which maximizes \(Q(\theta|\theta_t)\). \(\theta_{t+1}=\underset{g\epsilon G}{\mathrm{argmax}}{Q(\theta|\theta_t)}\). Deriving this value for the parameter \(\pi_{g,t+1}\) can be done by maximizing \(Q(\theta|\theta_t)\) with respect to \(\pi_{g,t+1}\) by \(\frac{\partial Q(\theta|\theta_t)}{\pi_{gt}}=0\), while subject to the constraint of \(\sum_{g=1}^{G}\pi_g=1\). This finally yields to \(\pi_{g,t+1}=\frac{1}{N}\sum_{i=1}^{N}h_{gt}(x_i)\).

    \item If \(Q(\theta_{t+1}|\theta_{t}) - Q(\theta_{t}|\theta_{t-1}) \le \xi \) (\(\xi \) is the specified termination threshold), then proceed to the last step. Otherwise, go back to Step 2.
    
    \item The final parameter set \(\theta^* = \theta_{t+1}\) is the derived maximized likelihood estimate of \(\theta\).
    
\end{enumerate}







\section{EM Parameter Initialization}
\label{sec:emparaminit_ch3}

\subsection{Kmeans Clustering}
\label{sec:kmeans}

\subsection{Heirarchical Clustering}
\label{sec:hclust}

\subsection{Peak Detection}
\label{sec:peakdetection}

\section{Evaluation Metrics}
\label{sec:evalmetric_ch3}

\subsection{Target copies per partition - \(\lambda\)}
\label{sec:lambda}
To count the number of target copies in a sample assay, the count of target copies per partition is determined. A partition is positive if it contains 1 or more target copies, otherwise, it is negative. The number of target copies in a partition is assumed to have a Poisson distribution
\[ P(x) = \frac{\lambda^xe^{-\lambda}}{x!} \],

where \(x\) is the count of target copies. The average occupancy per partition can then be derived as follows:
\[ 
    \begin{aligned}
        1-P(x=0) &= P(x>0)\\ 
        1-e^{-\lambda} &= \frac{N_{pos}}{N_{tot}}\\ 
        e^{-\lambda} &= 1-\frac{N_{pos}}{N_{tot}}\\ 
        e^{-\lambda} &= \frac{N_{neg}}{N_{tot}}\\ 
        \hat{\lambda} &= -ln(\frac{N_{neg}}{N_{tot}})
    \end{aligned}
\]

where \(N_{tot}\),\(N_{pos}\),and \(N_{neg}\) is the count of the total, positive, and negative partitions in the assay, respectively. The target concentration ()



\subsection{Log-linear relationship of \(\lambda\) and dilution factor}
\label{sec:loglinear}

\subsection{Coefficient of Variation}
\label{sec:coeffofvar}
